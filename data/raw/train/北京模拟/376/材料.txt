材料一
1956年，科学家马文•明斯基首次提出了“人工智能”这个概念。1997年，名为“深蓝”的人工智能击败了国际象棋大师卡斯帕罗夫。那时候人类以为，围棋不会被人工智能破解，因为围棋需要的计算量太大了。一盘围棋可以出现多少种可能？这个数字等于目前观测到的全宇宙原子数的平方，再乘上100亿。
计算破解行不通，人工智能采用了两个新招数。第一招，“向人学”：“看”人类怎么下棋。第二招，“自学”：人工智能每天自己跟自己对弈100万局。在对弈过程中，人工智能还有两个策略：一是“看大局”，给每一个落子后的局面评分，直接排除掉那些明显拖累局面的“烂棋”；二是“看细节”，给每一个落子的后续发展打分，倘若推算几步后发现发展不妙，立刻抛弃这条路，不再继续。大局分和细节分，二者取个平均分，就是谷歌围棋人工智能AlphaGo的下棋思路。简单，但管用。2016年3月，AlphaGo以4:1战胜了九段棋手李世乭。
这场胜利震动了世界。因为AlphaGo下棋的方式太像人类了，如果只看棋谱不看思考时间，根本看不出对弈双方谁是人类。许多人都在惊呼：人工智能已经变得越来越像人了！
一旦人工智能可以模仿人脑，它将具备人类经过生命数十亿年演化才获得的大部分心智能力。物理学家霍金呼吁人们警惕人工智能即将对人类生存构成的威胁——不久前，他因此“荣获”了“卢德奖”，一个专门授予反科学、阻碍技术进步人士的“奖项”。
人工智能正在让一部分人失业，这的确是已经发生的事实。假如你从事的是程序性的工作，不管是体力还是脑力的，在自动化浪潮前，都有失业的危险。因此，理性的人应该努力转向人工智能还没有太大竞争优势的非程序性工作，比如需要具体评估每颗牙怎么补、与每个患者怎么沟通的牙医——假如有个牙科钻头在你的嘴里，相信你并不希望它握在机器人手中。
材料二
如果有一天，人类创造出了超人工智能，这个世界会是什么样呢？
许多对未来充满信心的人对此感到非常兴奋。很难想象会有什么问题是超级智能解决不了的。疾病、贫困、环境毁灭、各种不必要的苦难，这些都是超级智能可以应对解决的。最激动人心的一个设想是：人工智能可以帮助人类实现永生。仔细想想，我们的身体是原子构成的，衰老只是身体的组成物质用旧了，就像汽车开久了零件会用旧一样。如果人工智能可以操纵各种原子结构，就能拥有完美的修复技术，或者干脆直接生产新零件不断替换掉旧的，这辆车就能永远开下去。甚至，我们可以把大脑中所有的记忆数据直接转移到一个由超级智能制造出的全新的身体里。
但是，在另一些人眼里，人工智能对人类构成的威胁甚至超过核武器。一次采访中，对“人工智能未来是否会自行思考、进化并给人类带来灾难”这个问题，比尔•盖茨的回答是：“我属于对超人工智能充满担忧的阵营。如果机器能替代人类做事，而没有发展为超级智能，那么只要管理好这些机器，善于利用，似乎没有太大危险。但在几十年的发展之后，人工智能很可能强大到需要警惕的阶段。”物理学巨匠霍金认为，成功研发人工智能将成为人类历史上犯的最大错误，不幸的是，这也可能是最后一个错误。他说：“人工智能技术的研发将敲响人类灭绝的警钟。这项技术能够按照自己的意愿行事，并且以越来越快的速度自行重新设计。人类受限于缓慢的生物学进化速度，无法与之竞争和对抗，最终将被取代。”
近日，包括霍金在内的数百名专家在一封公开信上联合签名，呼吁国际社会对人工智能研究进行合理监管，确保最前沿的研究不会失控。公开信指出，人工智能研究正快速取得进步，将对整个社会产生越来越大的影响。专家们警告说：尽管现有的人工智能研究成果给世界带来了更多便利，但是也应该警惕人工智能可能对人类社会造成的潜在伤害。科学家需要采取措施，预防最糟糕的结果出现。
忧也罢，乐也罢，毋庸置疑的是，无论是从现有的实际应用，还是从未来的发展趋势看，人工智能都是一把双刃剑。